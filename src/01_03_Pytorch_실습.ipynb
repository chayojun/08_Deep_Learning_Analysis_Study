{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80fa0a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0    0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1    0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2    0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3    0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4    0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "..       ...   ...    ...   ...    ...    ...   ...     ...  ...    ...   \n",
       "501  0.06263   0.0  11.93   0.0  0.573  6.593  69.1  2.4786  1.0  273.0   \n",
       "502  0.04527   0.0  11.93   0.0  0.573  6.120  76.7  2.2875  1.0  273.0   \n",
       "503  0.06076   0.0  11.93   0.0  0.573  6.976  91.0  2.1675  1.0  273.0   \n",
       "504  0.10959   0.0  11.93   0.0  0.573  6.794  89.3  2.3889  1.0  273.0   \n",
       "505  0.04741   0.0  11.93   0.0  0.573  6.030  80.8  2.5050  1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  MEDV  \n",
       "0       15.3  396.90   4.98  24.0  \n",
       "1       17.8  396.90   9.14  21.6  \n",
       "2       17.8  392.83   4.03  34.7  \n",
       "3       18.7  394.63   2.94  33.4  \n",
       "4       18.7  396.90   5.33  36.2  \n",
       "..       ...     ...    ...   ...  \n",
       "501     21.0  391.99   9.67  22.4  \n",
       "502     21.0  396.90   9.08  20.6  \n",
       "503     21.0  396.90   5.64  23.9  \n",
       "504     21.0  393.45   6.48  22.0  \n",
       "505     21.0  396.90   7.88  11.9  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/boston.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f122eaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   CRIM     506 non-null    float64\n",
      " 1   ZN       506 non-null    float64\n",
      " 2   INDUS    506 non-null    float64\n",
      " 3   CHAS     506 non-null    float64\n",
      " 4   NOX      506 non-null    float64\n",
      " 5   RM       506 non-null    float64\n",
      " 6   AGE      506 non-null    float64\n",
      " 7   DIS      506 non-null    float64\n",
      " 8   RAD      506 non-null    float64\n",
      " 9   TAX      506 non-null    float64\n",
      " 10  PTRATIO  506 non-null    float64\n",
      " 11  B        506 non-null    float64\n",
      " 12  LSTAT    506 non-null    float64\n",
      " 13  MEDV     506 non-null    float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 55.5 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5f3dc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X_train = df.iloc[:,:13].values\n",
    "#X_train = torch.tensor(X_train)\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "\n",
    "y_train = df.iloc[:,-1].values\n",
    "y_train = torch.FloatTensor(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3475527e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=13, out_features=100, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim.adam import Adam\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(13,100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100,50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50,1)\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a6c265e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 0.weight\n",
      "Shape: torch.Size([100, 13])\n",
      "Values: Parameter containing:\n",
      "tensor([[ 0.2411,  0.0898,  0.0007,  ...,  0.2103, -0.1179, -0.2117],\n",
      "        [-0.2289,  0.1218, -0.0105,  ..., -0.1075,  0.2596, -0.1480],\n",
      "        [-0.0396,  0.1829,  0.1489,  ...,  0.2283,  0.1598, -0.0970],\n",
      "        ...,\n",
      "        [-0.1950,  0.1136,  0.2386,  ..., -0.2618, -0.0892,  0.1496],\n",
      "        [-0.2264,  0.2548, -0.2134,  ...,  0.0022, -0.1483, -0.0138],\n",
      "        [ 0.0780, -0.0208, -0.0980,  ..., -0.0766, -0.1924, -0.2324]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Name: 0.bias\n",
      "Shape: torch.Size([100])\n",
      "Values: Parameter containing:\n",
      "tensor([-1.5356e-01, -9.2209e-03, -1.4203e-01,  6.4857e-02, -7.4510e-02,\n",
      "         1.9467e-01,  4.2868e-02, -2.1670e-01,  3.7495e-02,  1.7886e-01,\n",
      "        -5.2199e-02,  2.3551e-01,  4.1251e-02,  1.9002e-01, -2.1150e-01,\n",
      "        -2.2807e-01,  8.4788e-05, -3.7539e-02,  2.7594e-01,  1.4317e-03,\n",
      "        -1.3753e-01,  3.8800e-02, -9.9853e-02, -6.0627e-02, -8.0796e-02,\n",
      "         6.6171e-02, -2.5179e-01,  2.5800e-01, -2.3292e-01, -1.5229e-01,\n",
      "         1.1283e-01,  1.1202e-01, -1.5005e-01, -1.3984e-01,  4.8821e-02,\n",
      "        -3.9756e-02,  1.4910e-02,  1.3559e-01,  1.0252e-01,  2.4952e-01,\n",
      "        -2.4155e-01,  1.2147e-01,  1.2384e-01, -5.9814e-02,  7.8178e-02,\n",
      "        -2.6281e-01,  2.8927e-02, -7.5304e-02, -1.4394e-02, -6.7696e-02,\n",
      "         2.7581e-01, -9.3512e-02, -2.1679e-01,  1.0774e-01, -2.6113e-01,\n",
      "         1.5566e-01,  4.5942e-02,  6.4708e-02, -1.3938e-01,  2.5461e-01,\n",
      "        -2.3393e-01, -1.8703e-01,  7.0507e-02, -2.5586e-01, -2.8643e-02,\n",
      "         4.0244e-02,  1.9430e-01,  1.0742e-01,  9.0290e-02,  1.0976e-01,\n",
      "         1.0762e-01, -1.0070e-01, -8.4602e-02,  6.1242e-03,  1.3746e-01,\n",
      "        -2.3533e-01,  1.1019e-01,  1.4629e-01,  1.4413e-01, -2.1634e-01,\n",
      "        -1.0498e-01, -1.5067e-02, -3.1605e-02,  8.0457e-02, -1.9594e-01,\n",
      "         1.4618e-02,  1.5909e-01,  1.5289e-01, -2.5295e-01,  1.3651e-01,\n",
      "         1.3686e-01,  1.6264e-01, -4.3202e-02, -7.4591e-02,  1.0311e-02,\n",
      "         2.4056e-01,  1.3592e-01, -8.5706e-02, -2.5711e-01, -2.1751e-01],\n",
      "       requires_grad=True)\n",
      "\n",
      "Name: 2.weight\n",
      "Shape: torch.Size([50, 100])\n",
      "Values: Parameter containing:\n",
      "tensor([[-0.0430,  0.0472,  0.0643,  ...,  0.0148,  0.0474,  0.0455],\n",
      "        [ 0.0105,  0.0051, -0.0958,  ..., -0.0028, -0.0138, -0.0200],\n",
      "        [-0.0407,  0.0472,  0.0431,  ...,  0.0603, -0.0697,  0.0406],\n",
      "        ...,\n",
      "        [-0.0305, -0.0209, -0.0505,  ..., -0.0958, -0.0138, -0.0814],\n",
      "        [-0.0842,  0.0485, -0.0959,  ..., -0.0929, -0.0100,  0.0302],\n",
      "        [-0.0150, -0.0419, -0.0465,  ...,  0.0846, -0.0237,  0.0866]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Name: 2.bias\n",
      "Shape: torch.Size([50])\n",
      "Values: Parameter containing:\n",
      "tensor([-0.0731, -0.0208, -0.0503,  0.0755, -0.0887, -0.0719,  0.0028, -0.0126,\n",
      "         0.0918,  0.0729,  0.0661,  0.0502,  0.0239, -0.0208,  0.0885, -0.0780,\n",
      "         0.0766, -0.0718, -0.0917,  0.0135,  0.0469, -0.0482, -0.0051, -0.0400,\n",
      "        -0.0499,  0.0365,  0.0135,  0.0710,  0.0645, -0.0043,  0.0758, -0.0592,\n",
      "        -0.0283, -0.0235, -0.0544,  0.0690,  0.0619, -0.0247, -0.0185, -0.0725,\n",
      "        -0.0191,  0.0513, -0.0033,  0.0760, -0.0933,  0.0684,  0.0877,  0.0727,\n",
      "        -0.0875,  0.0182], requires_grad=True)\n",
      "\n",
      "Name: 4.weight\n",
      "Shape: torch.Size([1, 50])\n",
      "Values: Parameter containing:\n",
      "tensor([[-0.0290, -0.0638, -0.0779,  0.0006,  0.0996, -0.1152,  0.0426, -0.0324,\n",
      "         -0.0980, -0.0308, -0.0549,  0.0836,  0.0011,  0.0066,  0.0435, -0.0253,\n",
      "          0.0778, -0.1157,  0.0039,  0.0602, -0.0443, -0.0954, -0.0951,  0.0536,\n",
      "          0.0123, -0.1149,  0.0103,  0.1104, -0.0434, -0.0541,  0.0972,  0.0383,\n",
      "         -0.1211, -0.0816,  0.1069,  0.0576, -0.1206, -0.0916, -0.0342,  0.1286,\n",
      "         -0.0282,  0.0880,  0.0213, -0.0098, -0.1314,  0.0274, -0.0360, -0.0676,\n",
      "         -0.0970,  0.0619]], requires_grad=True)\n",
      "\n",
      "Name: 4.bias\n",
      "Shape: torch.Size([1])\n",
      "Values: Parameter containing:\n",
      "tensor([0.0147], requires_grad=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Name: {name}\")\n",
    "    print(f\"Shape: {param.shape}\")\n",
    "    print(f\"Values: {param}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b78e658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  1586.50341796875\n",
      "loss :  1029.7838134765625\n",
      "loss :  620.5833740234375\n",
      "loss :  339.564697265625\n",
      "loss :  168.02198791503906\n",
      "loss :  97.31220245361328\n",
      "loss :  104.77114868164062\n",
      "loss :  157.63475036621094\n",
      "loss :  218.53753662109375\n",
      "loss :  259.8060607910156\n",
      "loss :  272.995849609375\n",
      "loss :  263.9592590332031\n",
      "loss :  238.94732666015625\n",
      "loss :  205.49159240722656\n",
      "loss :  170.75180053710938\n",
      "loss :  141.22459411621094\n",
      "loss :  119.09840393066406\n",
      "loss :  104.12287139892578\n",
      "loss :  95.29659271240234\n",
      "loss :  91.86137390136719\n",
      "loss :  92.5843276977539\n",
      "loss :  96.10360717773438\n",
      "loss :  101.03744506835938\n",
      "loss :  106.12104034423828\n",
      "loss :  110.35691833496094\n",
      "loss :  113.06779479980469\n",
      "loss :  113.92658233642578\n",
      "loss :  112.94369506835938\n",
      "loss :  110.41405487060547\n",
      "loss :  106.81401062011719\n",
      "loss :  102.70881652832031\n",
      "loss :  98.64321899414062\n",
      "loss :  95.08275604248047\n",
      "loss :  92.35115814208984\n",
      "loss :  90.60894012451172\n",
      "loss :  89.8493881225586\n",
      "loss :  89.92745971679688\n",
      "loss :  90.59886932373047\n",
      "loss :  91.5694580078125\n",
      "loss :  92.55143737792969\n",
      "loss :  93.30934143066406\n",
      "loss :  93.68598937988281\n",
      "loss :  93.61979675292969\n",
      "loss :  93.13880157470703\n",
      "loss :  92.34115600585938\n",
      "loss :  91.36720275878906\n",
      "loss :  90.36800384521484\n",
      "loss :  89.47722625732422\n",
      "loss :  88.79084777832031\n",
      "loss :  88.3566665649414\n",
      "loss :  88.1727066040039\n",
      "loss :  88.19557189941406\n",
      "loss :  88.35458374023438\n",
      "loss :  88.56864929199219\n",
      "loss :  88.76268768310547\n",
      "loss :  88.8791275024414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\POTENUP\\08_Deep_Learning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:616: UserWarning: Using a target size (torch.Size([506])) that is different to the input size (torch.Size([506, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  88.88709259033203\n",
      "loss :  88.78242492675781\n",
      "loss :  88.58491516113281\n",
      "loss :  88.32990264892578\n",
      "loss :  88.05941772460938\n",
      "loss :  87.81281280517578\n",
      "loss :  87.6208267211914\n",
      "loss :  87.4986343383789\n",
      "loss :  87.44623565673828\n",
      "loss :  87.45081329345703\n",
      "loss :  87.49097442626953\n",
      "loss :  87.54231262207031\n",
      "loss :  87.58311462402344\n",
      "loss :  87.59829711914062\n",
      "loss :  87.58135986328125\n",
      "loss :  87.53426361083984\n",
      "loss :  87.4656982421875\n",
      "loss :  87.38807678222656\n",
      "loss :  87.31415557861328\n",
      "loss :  87.25418090820312\n",
      "loss :  87.21401977539062\n",
      "loss :  87.1944808959961\n",
      "loss :  87.19204711914062\n",
      "loss :  87.20023345947266\n",
      "loss :  87.21165466308594\n",
      "loss :  87.21971893310547\n",
      "loss :  87.21991729736328\n",
      "loss :  87.21054077148438\n",
      "loss :  87.19265747070312\n",
      "loss :  87.16927337646484\n",
      "loss :  87.14434051513672\n",
      "loss :  87.12162017822266\n",
      "loss :  87.10387420654297\n",
      "loss :  87.09223175048828\n",
      "loss :  87.08634948730469\n",
      "loss :  87.0845947265625\n",
      "loss :  87.08473205566406\n",
      "loss :  87.08445739746094\n",
      "loss :  87.08206939697266\n",
      "loss :  87.07674407958984\n",
      "loss :  87.06857299804688\n",
      "loss :  87.05840301513672\n",
      "loss :  87.04745483398438\n",
      "loss :  87.03697204589844\n",
      "loss :  87.02790832519531\n",
      "loss :  87.0207290649414\n",
      "loss :  87.01531982421875\n",
      "loss :  87.01116180419922\n",
      "loss :  87.00753021240234\n",
      "loss :  87.00370788574219\n",
      "loss :  86.99917602539062\n",
      "loss :  86.99374389648438\n",
      "loss :  86.98743438720703\n",
      "loss :  86.98057556152344\n",
      "loss :  86.97355651855469\n",
      "loss :  86.966796875\n",
      "loss :  86.96053314208984\n",
      "loss :  86.95480346679688\n",
      "loss :  86.94957733154297\n",
      "loss :  86.94469451904297\n",
      "loss :  86.93987274169922\n",
      "loss :  86.93494415283203\n",
      "loss :  86.92960357666016\n",
      "loss :  86.92382049560547\n",
      "loss :  86.91777801513672\n",
      "loss :  86.91159057617188\n",
      "loss :  86.9054183959961\n",
      "loss :  86.89938354492188\n",
      "loss :  86.89354705810547\n",
      "loss :  86.88788604736328\n",
      "loss :  86.88233947753906\n",
      "loss :  86.8768081665039\n",
      "loss :  86.87084197998047\n",
      "loss :  86.8647689819336\n",
      "loss :  86.85858917236328\n",
      "loss :  86.8523178100586\n",
      "loss :  86.84600830078125\n",
      "loss :  86.83968353271484\n",
      "loss :  86.83336639404297\n",
      "loss :  86.82711029052734\n",
      "loss :  86.82091522216797\n",
      "loss :  86.81475830078125\n",
      "loss :  86.80859375\n",
      "loss :  86.80242156982422\n",
      "loss :  86.79619598388672\n",
      "loss :  86.78975677490234\n",
      "loss :  86.78324890136719\n",
      "loss :  86.7767105102539\n",
      "loss :  86.7701644897461\n",
      "loss :  86.76361846923828\n",
      "loss :  86.75707244873047\n",
      "loss :  86.75055694580078\n",
      "loss :  86.74394989013672\n",
      "loss :  86.7373046875\n",
      "loss :  86.73070526123047\n",
      "loss :  86.72429656982422\n",
      "loss :  86.7178955078125\n",
      "loss :  86.71151733398438\n",
      "loss :  86.7051010131836\n",
      "loss :  86.69869232177734\n",
      "loss :  86.69229125976562\n",
      "loss :  86.68594360351562\n",
      "loss :  86.67950439453125\n",
      "loss :  86.67301177978516\n",
      "loss :  86.66639709472656\n",
      "loss :  86.6597671508789\n",
      "loss :  86.65299987792969\n",
      "loss :  86.64620971679688\n",
      "loss :  86.63929748535156\n",
      "loss :  86.6321792602539\n",
      "loss :  86.62515258789062\n",
      "loss :  86.61808776855469\n",
      "loss :  86.61087799072266\n",
      "loss :  86.60382080078125\n",
      "loss :  86.59651184082031\n",
      "loss :  86.58906555175781\n",
      "loss :  86.58167266845703\n",
      "loss :  86.57415008544922\n",
      "loss :  86.56614685058594\n",
      "loss :  86.55747985839844\n",
      "loss :  86.54830169677734\n",
      "loss :  86.53870391845703\n",
      "loss :  86.52877044677734\n",
      "loss :  86.51830291748047\n",
      "loss :  86.5087890625\n",
      "loss :  86.50035095214844\n",
      "loss :  86.49293518066406\n",
      "loss :  86.48600769042969\n",
      "loss :  86.47967529296875\n",
      "loss :  86.47398376464844\n",
      "loss :  86.46847534179688\n",
      "loss :  86.46295928955078\n",
      "loss :  86.45748138427734\n",
      "loss :  86.4519271850586\n",
      "loss :  86.44634246826172\n",
      "loss :  86.44075775146484\n",
      "loss :  86.43516540527344\n",
      "loss :  86.42909240722656\n",
      "loss :  86.42265319824219\n",
      "loss :  86.41613006591797\n",
      "loss :  86.40953826904297\n",
      "loss :  86.40296173095703\n",
      "loss :  86.39611053466797\n",
      "loss :  86.38907623291016\n"
     ]
    }
   ],
   "source": [
    "optim = Adam(model.parameters(), lr=0.001)\n",
    "epochs = 200\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 순서가 정말 중요!!!\n",
    "    optim.zero_grad() \n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred,y_train)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    print(\"loss : \",  loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6418acbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[ 0.2360,  0.0744,  0.0039,  ...,  0.2136, -0.1227, -0.2080],\n",
       "                      [-0.2157,  0.1123, -0.0159,  ..., -0.1159,  0.2517, -0.1533],\n",
       "                      [-0.0377,  0.1727,  0.1406,  ...,  0.2189,  0.1520, -0.1053],\n",
       "                      ...,\n",
       "                      [-0.1867,  0.1056,  0.2316,  ..., -0.2703, -0.0946,  0.1431],\n",
       "                      [-0.2264,  0.2548, -0.2134,  ...,  0.0022, -0.1483, -0.0138],\n",
       "                      [ 0.0674, -0.0208, -0.1106,  ..., -0.0894, -0.2035, -0.2449]])),\n",
       "             ('0.bias',\n",
       "              tensor([-0.1491, -0.0186, -0.1524,  0.0742, -0.0700,  0.1973,  0.0520, -0.2286,\n",
       "                       0.0252,  0.1695, -0.0522,  0.2355,  0.0312,  0.1900, -0.2246, -0.2404,\n",
       "                      -0.0182, -0.0375,  0.2834, -0.0093, -0.1375,  0.0388, -0.0739, -0.0540,\n",
       "                      -0.0808,  0.0726, -0.2612,  0.2615, -0.2256, -0.1523,  0.1079,  0.1120,\n",
       "                      -0.1501, -0.1398,  0.0488, -0.0398,  0.0272,  0.1460,  0.0905,  0.2521,\n",
       "                      -0.2416,  0.1215,  0.1192, -0.0531,  0.0675, -0.2672,  0.0290, -0.0753,\n",
       "                      -0.0369, -0.0631,  0.2661, -0.0834, -0.2168,  0.0948, -0.2166,  0.1557,\n",
       "                       0.0528,  0.0544, -0.1377,  0.2600, -0.2277, -0.2031,  0.0560, -0.2443,\n",
       "                      -0.0543,  0.0382,  0.2165,  0.1074,  0.0903,  0.0927,  0.0967, -0.1159,\n",
       "                      -0.0751,  0.0066,  0.1232, -0.2471,  0.1189,  0.1348,  0.1344, -0.2163,\n",
       "                      -0.1161, -0.0300, -0.0325,  0.0731, -0.1959,  0.0146,  0.1621,  0.1381,\n",
       "                      -0.2529,  0.1316,  0.1369,  0.1626, -0.0325, -0.0853,  0.0266,  0.2502,\n",
       "                       0.1359, -0.0959, -0.2571, -0.2304])),\n",
       "             ('2.weight',\n",
       "              tensor([[-0.0552,  0.0366,  0.0532,  ...,  0.0034,  0.0474,  0.0336],\n",
       "                      [-0.0012, -0.0067, -0.1071,  ..., -0.0138, -0.0138, -0.0277],\n",
       "                      [-0.0521,  0.0390,  0.0329,  ...,  0.0493, -0.0697,  0.0284],\n",
       "                      ...,\n",
       "                      [-0.0389, -0.0278, -0.0578,  ..., -0.1032, -0.0138, -0.0883],\n",
       "                      [-0.0842,  0.0485, -0.0959,  ..., -0.0929, -0.0100,  0.0302],\n",
       "                      [-0.0150, -0.0419, -0.0465,  ...,  0.0846, -0.0237,  0.0866]])),\n",
       "             ('2.bias',\n",
       "              tensor([-0.0842, -0.0322, -0.0603,  0.0755, -0.0786, -0.0806,  0.0125, -0.0226,\n",
       "                       0.0816,  0.0628,  0.0661,  0.0502,  0.0239, -0.0184,  0.0971, -0.0882,\n",
       "                       0.0766, -0.0821, -0.0962,  0.0233,  0.0469, -0.0482, -0.0051, -0.0400,\n",
       "                      -0.0499,  0.0261,  0.0135,  0.0705,  0.0645, -0.0140,  0.0742, -0.0636,\n",
       "                      -0.0388, -0.0340, -0.0443,  0.0690,  0.0509, -0.0247, -0.0185, -0.0622,\n",
       "                      -0.0299,  0.0482, -0.0033,  0.0760, -0.1044,  0.0684,  0.0772,  0.0655,\n",
       "                      -0.0875,  0.0182])),\n",
       "             ('4.weight',\n",
       "              tensor([[-0.0195, -0.0532, -0.0678,  0.0006,  0.1024, -0.1077,  0.0462, -0.0218,\n",
       "                       -0.0895, -0.0221, -0.0549,  0.0836,  0.0011,  0.0088,  0.0395, -0.0164,\n",
       "                        0.0778, -0.1051, -0.0002,  0.0627, -0.0443, -0.0954, -0.0951,  0.0536,\n",
       "                        0.0123, -0.1064,  0.0103,  0.1052, -0.0434, -0.0451,  0.0913,  0.0338,\n",
       "                       -0.1134, -0.0743,  0.1093,  0.0576, -0.1144, -0.0916, -0.0342,  0.1331,\n",
       "                       -0.0194,  0.0797,  0.0213, -0.0098, -0.1210,  0.0274, -0.0269, -0.0611,\n",
       "                       -0.0970,  0.0619]])),\n",
       "             ('4.bias', tensor([0.0251]))])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "58581701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4ae2e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  86.3820571899414\n",
      "loss :  86.65125274658203\n",
      "loss :  86.52328491210938\n",
      "loss :  89.90563201904297\n",
      "loss :  87.09339141845703\n",
      "loss :  89.5189208984375\n",
      "loss :  86.77336120605469\n",
      "loss :  86.81427764892578\n",
      "loss :  88.33548736572266\n",
      "loss :  87.11885833740234\n",
      "loss :  86.29568481445312\n",
      "loss :  87.24727630615234\n",
      "loss :  87.32537841796875\n",
      "loss :  86.34407806396484\n",
      "loss :  86.27946472167969\n",
      "loss :  86.95538330078125\n",
      "loss :  86.80448913574219\n",
      "loss :  86.21959686279297\n",
      "loss :  86.3359375\n",
      "loss :  86.70252227783203\n",
      "loss :  86.45482635498047\n",
      "loss :  86.08718872070312\n",
      "loss :  86.24908447265625\n",
      "loss :  86.4614486694336\n",
      "loss :  86.2509536743164\n",
      "loss :  86.07780456542969\n",
      "loss :  86.2396469116211\n",
      "loss :  86.29861450195312\n",
      "loss :  86.0916748046875\n",
      "loss :  86.02745056152344\n",
      "loss :  86.16889953613281\n",
      "loss :  86.15418243408203\n",
      "loss :  86.01823425292969\n",
      "loss :  86.04227447509766\n",
      "loss :  86.11329650878906\n",
      "loss :  86.03031158447266\n",
      "loss :  85.95833587646484\n",
      "loss :  86.01771545410156\n",
      "loss :  86.02974700927734\n",
      "loss :  85.95454406738281\n",
      "loss :  85.95215606689453\n",
      "loss :  85.98793029785156\n",
      "loss :  85.94322967529297\n",
      "loss :  85.90460205078125\n",
      "loss :  85.9344711303711\n",
      "loss :  85.93008422851562\n",
      "loss :  85.88785552978516\n",
      "loss :  85.89193725585938\n",
      "loss :  85.89971923828125\n",
      "loss :  85.8656997680664\n",
      "loss :  85.85492706298828\n",
      "loss :  85.86817932128906\n",
      "loss :  85.849365234375\n",
      "loss :  85.82992553710938\n",
      "loss :  85.83540344238281\n",
      "loss :  85.82362365722656\n",
      "loss :  85.80529022216797\n",
      "loss :  85.80792236328125\n",
      "loss :  85.80199432373047\n",
      "loss :  85.78388977050781\n",
      "loss :  85.78035736083984\n",
      "loss :  85.77510070800781\n",
      "loss :  85.76063537597656\n",
      "loss :  85.75659942626953\n",
      "loss :  85.75263977050781\n",
      "loss :  85.73896026611328\n",
      "loss :  85.73253631591797\n",
      "loss :  85.72832489013672\n",
      "loss :  85.71783447265625\n",
      "loss :  85.71168518066406\n",
      "loss :  85.7061538696289\n",
      "loss :  85.6958236694336\n",
      "loss :  85.69014739990234\n",
      "loss :  85.68463134765625\n",
      "loss :  85.67542266845703\n",
      "loss :  85.66929626464844\n",
      "loss :  85.6632308959961\n",
      "loss :  85.65494537353516\n",
      "loss :  85.64932250976562\n",
      "loss :  85.64288330078125\n",
      "loss :  85.63545227050781\n",
      "loss :  85.63015747070312\n",
      "loss :  85.62347412109375\n",
      "loss :  85.61683654785156\n",
      "loss :  85.61134338378906\n",
      "loss :  85.60470581054688\n",
      "loss :  85.59878540039062\n",
      "loss :  85.59326171875\n",
      "loss :  85.58671569824219\n",
      "loss :  85.58121490478516\n",
      "loss :  85.57553100585938\n",
      "loss :  85.56951904296875\n",
      "loss :  85.5643081665039\n",
      "loss :  85.55841827392578\n",
      "loss :  85.55276489257812\n",
      "loss :  85.54743194580078\n",
      "loss :  85.54170227050781\n",
      "loss :  85.53636169433594\n",
      "loss :  85.53094482421875\n",
      "loss :  85.52542877197266\n",
      "loss :  85.5202407836914\n",
      "loss :  85.5148696899414\n",
      "loss :  85.50962829589844\n",
      "loss :  85.50443267822266\n",
      "loss :  85.49910736083984\n",
      "loss :  85.49405670166016\n",
      "loss :  85.4888916015625\n",
      "loss :  85.48375701904297\n",
      "loss :  85.47879028320312\n",
      "loss :  85.47370147705078\n",
      "loss :  85.4687271118164\n",
      "loss :  85.46375274658203\n",
      "loss :  85.4587631225586\n",
      "loss :  85.45393371582031\n",
      "loss :  85.44902801513672\n",
      "loss :  85.4442367553711\n",
      "loss :  85.439453125\n",
      "loss :  85.43464660644531\n",
      "loss :  85.4299087524414\n",
      "loss :  85.4251480102539\n",
      "loss :  85.42044830322266\n",
      "loss :  85.41571044921875\n",
      "loss :  85.41102600097656\n",
      "loss :  85.40641784667969\n",
      "loss :  85.40177154541016\n",
      "loss :  85.3971176147461\n",
      "loss :  85.3924789428711\n",
      "loss :  85.38790130615234\n",
      "loss :  85.38325500488281\n",
      "loss :  85.37866973876953\n",
      "loss :  85.37406158447266\n",
      "loss :  85.3694839477539\n",
      "loss :  85.36494445800781\n",
      "loss :  85.36039733886719\n",
      "loss :  85.3558578491211\n",
      "loss :  85.35135650634766\n",
      "loss :  85.3470230102539\n",
      "loss :  85.34270477294922\n",
      "loss :  85.33839416503906\n",
      "loss :  85.33413696289062\n",
      "loss :  85.32984161376953\n",
      "loss :  85.3255844116211\n",
      "loss :  85.32131958007812\n",
      "loss :  85.31706237792969\n",
      "loss :  85.31285095214844\n",
      "loss :  85.3086166381836\n",
      "loss :  85.30443572998047\n",
      "loss :  85.30022430419922\n",
      "loss :  85.2959976196289\n",
      "loss :  85.29182434082031\n",
      "loss :  85.28772735595703\n",
      "loss :  85.28362274169922\n",
      "loss :  85.27952575683594\n",
      "loss :  85.27539825439453\n",
      "loss :  85.27132415771484\n",
      "loss :  85.26728820800781\n",
      "loss :  85.26325225830078\n",
      "loss :  85.25929260253906\n",
      "loss :  85.25536346435547\n",
      "loss :  85.25139617919922\n",
      "loss :  85.24748229980469\n",
      "loss :  85.2436294555664\n",
      "loss :  85.23973846435547\n",
      "loss :  85.23590850830078\n",
      "loss :  85.23212432861328\n",
      "loss :  85.22836303710938\n",
      "loss :  85.22463989257812\n",
      "loss :  85.22090911865234\n",
      "loss :  85.2171859741211\n",
      "loss :  85.21345520019531\n",
      "loss :  85.20979309082031\n",
      "loss :  85.2060546875\n",
      "loss :  85.20236206054688\n",
      "loss :  85.19867706298828\n",
      "loss :  85.19500732421875\n",
      "loss :  85.19136810302734\n",
      "loss :  85.18778991699219\n",
      "loss :  85.1844253540039\n",
      "loss :  85.1808090209961\n",
      "loss :  85.1773452758789\n",
      "loss :  85.17384338378906\n",
      "loss :  85.17033386230469\n",
      "loss :  85.16688537597656\n",
      "loss :  85.1634292602539\n",
      "loss :  85.15999603271484\n",
      "loss :  85.15677642822266\n",
      "loss :  85.15331268310547\n",
      "loss :  85.15010833740234\n",
      "loss :  85.14678192138672\n",
      "loss :  85.14346313476562\n",
      "loss :  85.1402359008789\n",
      "loss :  85.13691711425781\n",
      "loss :  85.13365173339844\n",
      "loss :  85.13043212890625\n",
      "loss :  85.12723541259766\n",
      "loss :  85.12387084960938\n",
      "loss :  85.12059783935547\n",
      "loss :  85.11751556396484\n",
      "loss :  85.11415100097656\n",
      "loss :  85.11089324951172\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "optim = Adam(model.parameters(), lr=0.001)\n",
    "epochs = 200\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 순서가 정말 중요!!!\n",
    "    optim.zero_grad() \n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred,y_train)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    print(\"loss : \",  loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "08-deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
